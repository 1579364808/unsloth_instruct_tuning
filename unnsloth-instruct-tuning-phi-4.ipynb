{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T09:27:32.597370Z","iopub.execute_input":"2025-03-23T09:27:32.597692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 示例脚本，演示如何使用来自Hugging Face的思维链(COT)数据集\n\nimport torch\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom tqdm import tqdm\nimport re\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 直接从Hugging Face加载COT数据集\nprint(\"正在从Hugging Face加载COT数据集...\")\ndataset_name = \"MelodyOfTears/sst2-with-cot\"  # 替换为你的数据集名称\nenhanced_dataset = load_dataset(dataset_name, split=\"train\")\nprint(f\"已加载{len(enhanced_dataset)}个带有思维链推理的示例\")\n\n# 初始化模型\nmax_seq_length = 2048\ndtype = torch.float16  # 用于4位量化\nload_in_4bit = True\n\nprint(\"正在加载模型...\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/phi-4\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义训练用的提示模板\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with\nan input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by\nstep chain of thoughts to ensure a logical and accurate response.\n### Instruction:\nYou are an expert in sentiment analysis with advanced knowledge in understanding\nand interpreting emotions from text.\nPlease analyze the sentiment of the following text and output 0 (negative) or 1 (positive).\n### Text:\n{}\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n\n# 格式化数据集用于训练\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"sentence\"]\n    cots = examples[\"Complex_CoT\"]\n    outputs = examples[\"label\"]\n    texts = []\n    for input, cot, output in zip(inputs, cots, outputs):\n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n        texts.append(text)\n    return {\n        \"text\": texts,\n    }\n\nprint(\"正在格式化数据集用于训练...\")\nformatted_dataset = enhanced_dataset.map(formatting_prompts_func, batched=True)\n\n# 打印一个示例进行验证\nprint(\"\\n格式化训练数据的示例:\")\nprint(formatted_dataset[0]['text'][:0] + \"...\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 设置LoRA用于微调\nprint(\"\\n正在设置LoRA用于微调...\")\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# 设置训练器\nprint(\"正在设置训练器...\")\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=formatted_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=1,\n        num_train_epochs=1,\n        warmup_ratio=0.1,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\"  # 禁用wandb\n    ),\n)\n\n# 开始训练\nprint(\"\\n开始训练...\")\ntrainer_stats = trainer.train()\nprint(\"训练完成！\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with\nan input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by\nstep chain of thoughts to ensure a logical and accurate response.\n### Instruction:\nYou are an expert in sentiment analysis with advanced knowledge in understanding\nand interpreting emotions from text.\nPlease analyze the sentiment of the following text and output 0 (negative) or 1 (positive).\n### Text:\n{}\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 定义推理函数\ndef inference_example(text):\n    FastLanguageModel.for_inference(model)\n    inputs = tokenizer([prompt_style.format(text, \"\")], return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=1200,\n        use_cache=True,\n    )\n    response = tokenizer.batch_decode(outputs)\n    return response[0].split(\"### Response:\")[1]\n\n# 测试模型\nprint(\"\\n使用示例测试模型:\")\ntest_text = \"I absolutely loved the movie! The acting was superb and the storyline was captivating.\"\ntest_text = \"I hate you!\"\nprint(f\"文本: {test_text}\")\nprint(f\"预测结果: {inference_example(test_text)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 在SST2验证集上评估模型\ndef evaluate_model_on_sst2():\n    # 加载SST2验证集\n    print(\"\\n正在加载SST2验证集...\")\n    validation_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n\n    # 将模型设置为推理模式\n    FastLanguageModel.for_inference(model)\n\n    # 初始化计数器\n    correct_predictions = 0\n    total_predictions = 0\n\n    # 创建进度条\n    progress_bar = tqdm(validation_dataset, desc=\"正在评估SST2验证集\")\n\n    # 评估验证集中的每个样本\n    for sample in progress_bar:\n        text = sample[\"sentence\"]\n        true_label = sample[\"label\"]\n\n        # 使用模型进行预测\n        try:\n            # 获取模型输出\n            inputs = tokenizer([prompt_style.format(text, \"\")], return_tensors=\"pt\").to(\"cuda\")\n            outputs = model.generate(\n                input_ids=inputs.input_ids,\n                attention_mask=inputs.attention_mask,\n                max_new_tokens=300,  # 减少token数量以加快评估速度\n                use_cache=True,\n            )\n                        # 提取预测标签（0或1），从<think>标签外的内容中提取\n            response_after_think = tokenizer.batch_decode(outputs)[0].split(\"</think>\")[1].strip()\n            pred_label_match = re.search(r'[01]', response_after_think)\n\n            if pred_label_match:\n                pred_label = int(pred_label_match.group())\n\n                # 更新计数器\n                if pred_label == true_label:\n                    correct_predictions += 1\n                total_predictions += 1\n\n                # 更新进度条描述以显示当前准确率\n                current_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n                progress_bar.set_description(f\"准确率: {current_accuracy:.4f}\")\n        except Exception as e:\n            print(f\"处理样本时出错: {e}\")\n\n    # 计算最终准确率\n    final_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n    print(f\"\\nSST2验证集上的评估结果:\")\n    print(f\"评估的样本总数: {total_predictions}\")\n    print(f\"正确预测数: {correct_predictions}\")\n    print(f\"准确率: {final_accuracy:.4f}\")\n\n    return final_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 运行评估\nprint(\"\\n开始在SST2验证集上进行评估...\")\naccuracy = evaluate_model_on_sst2()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}