{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:38:50.833887Z",
     "start_time": "2025-03-15T16:38:50.825632Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install unsloth openai datasets transformers trl\n",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:38:50.950364Z",
     "start_time": "2025-03-15T16:38:50.932539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/cache/'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:39:03.253876Z",
     "start_time": "2025-03-15T16:38:51.105012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:45.902165200Z",
     "start_time": "2025-03-15T16:39:03.316943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„ API Key\n",
    "api_key = os.environ.get(\"DeepSeek_API_KEY\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:39:31.748026Z",
     "start_time": "2025-03-15T16:39:03.437374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_seq_length = 2048\n",
    "dtype = torch.float16  # é€‚ç”¨äº 4-bit é‡åŒ–\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.10: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA vGPU-32GB. Num GPUs = 1. Max memory: 31.503 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:39:31.819448Z",
     "start_time": "2025-03-15T16:39:31.813417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with\n",
    "an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by\n",
    "step chain of thoughts to ensure a logical and accurate response.\n",
    "### Instruction:\n",
    "You are an expert in sentiment analysis with advanced knowledge in understanding\n",
    "and interpreting emotions from text.\n",
    "Please analyze the sentiment of the following text and output 0 (negative) or 1 (positive).\n",
    "### Text:\n",
    "{}\n",
    "### Response:\n",
    "<think>{}\"\"\"\n",
    "\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with\n",
    "an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by\n",
    "step chain of thoughts to ensure a logical and accurate response.\n",
    "### Instruction:\n",
    "You are an expert in sentiment analysis with advanced knowledge in understanding\n",
    "and interpreting emotions from text.\n",
    "Please analyze the sentiment of the following text and output 0 (negative) or 1 (positive).\n",
    "### Text:\n",
    "{} \n",
    "### Response: \n",
    "<think> \n",
    "{} \n",
    "</think> \n",
    "{}\"\"\"\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:39:31.914614Z",
     "start_time": "2025-03-15T16:39:31.906027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_cot(text, label, max_retries=3):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ€ç»´é“¾çš„å‡½æ•°\n",
    "    :param text: éœ€è¦åˆ†æçš„æ–‡æœ¬\n",
    "    :param label: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆ0/1ï¼‰\n",
    "    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "    :return: ç”Ÿæˆçš„CoTå­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    sentiment_map = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "    prompt = f\"\"\"As a sentiment analysis expert, generate a step-by-step Chain of Thought (CoT) in English to explain why the following text is {sentiment_map[label]}. \n",
    "The CoT should follow this structure:\n",
    "1. Identify key sentiment-bearing words/phrases\n",
    "2. Analyze contextual clues\n",
    "3. Consider linguistic patterns\n",
    "4. Synthesize overall sentiment\n",
    "5. Conclude with the final sentiment label (0 for negative, 1 for positive)\n",
    "\n",
    "Text: {text}\n",
    "CoT:\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-chat\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert in sentiment analysis and logical reasoning.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=300,\n",
    "                stream=False\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt+1} failed, retrying...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"Failed after {max_retries} attempts: {str(e)}\")\n",
    "                return \"CoT generation failed\"\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:39:32.012681Z",
     "start_time": "2025-03-15T16:39:32.005221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_cot_to_dataset(dataset, sample_size=None):\n",
    "    \"\"\"\n",
    "    ä¸ºæ•°æ®é›†æ·»åŠ CoTå­—æ®µ\n",
    "    :param dataset: åŸå§‹æ•°æ®é›†\n",
    "    :param sample_size: é‡‡æ ·æ•°é‡ï¼ˆæµ‹è¯•æ—¶ä½¿ç”¨ï¼‰\n",
    "    :return: åŒ…å«CoTçš„æ–°æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    if sample_size:\n",
    "        dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
    "\n",
    "    texts = dataset[\"sentence\"]  # SST-2æ•°æ®é›†çš„æ–‡æœ¬å­—æ®µæ˜¯\"sentence\"\n",
    "    labels = dataset[\"label\"]\n",
    "\n",
    "    cots = []\n",
    "    for text, label in tqdm(zip(texts, labels), total=len(texts)):\n",
    "        cot = generate_cot(text, label)\n",
    "        cots.append(cot)\n",
    "        time.sleep(1)  # æ§åˆ¶APIè°ƒç”¨é¢‘ç‡\n",
    "\n",
    "    return dataset.add_column(\"Complex_CoT\", cots)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:42:55.613032Z",
     "start_time": "2025-03-15T16:39:32.114559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:500]\")#ä½¿ç”¨500æ¡æ•°æ®è®­ç»ƒ\n",
    "\n",
    "# ç”ŸæˆCoTæ•°æ®\n",
    "enhanced_dataset = add_cot_to_dataset(dataset, sample_size=10)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:11<00:00, 19.13s/it]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:42:55.729158Z",
     "start_time": "2025-03-15T16:42:55.712018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"sentence\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"label\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "formatted_dataset = enhanced_dataset.map(formatting_prompts_func, batched=True)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1403.06 examples/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:42:55.830350Z",
     "start_time": "2025-03-15T16:42:55.822925Z"
    }
   },
   "cell_type": "code",
   "source": "print(formatted_dataset[0]['Complex_CoT'])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Identify key sentiment-bearing words/phrases**:  \n",
      "   - \"hide\" implies concealment or secrecy, which often carries a negative connotation.  \n",
      "   - \"new secretions\" is an unusual and somewhat unsettling phrase, suggesting something unnatural or undesirable.  \n",
      "   - \"parental units\" is a detached and impersonal way to refer to parents, which can indicate emotional distance or resentment.  \n",
      "\n",
      "2. **Analyze contextual clues**:  \n",
      "   - The act of hiding something from parents suggests a lack of trust or fear of judgment.  \n",
      "   - The phrase \"new secretions\" is ambiguous but evokes a sense of something being hidden because it is inappropriate, embarrassing, or harmful.  \n",
      "   - The use of \"parental units\" instead of \"parents\" adds a cold, clinical tone, further distancing the speaker emotionally.  \n",
      "\n",
      "3. **Consider linguistic patterns**:  \n",
      "   - The choice of words like \"hide\" and \"secretions\" creates a sense of discomfort or unease.  \n",
      "   - The impersonal term \"parental units\" dehumanizes the parents, which can reflect negative feelings toward them.  \n",
      "   - The overall phrasing is indirect and evasive, reinforcing the negative sentiment.  \n",
      "\n",
      "4. **Synthesize overall sentiment**:  \n",
      "   - The text conveys a sense of secrecy, discomfort, and emotional detachment.  \n",
      "   - The negative connotations of the key words and the context suggest an underlying tension or conflict.  \n",
      "\n",
      "5. **Conclude with the final sentiment label**:\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:42:55.921500Z",
     "start_time": "2025-03-15T16:42:55.916878Z"
    }
   },
   "cell_type": "code",
   "source": "print(formatted_dataset[0]['text'])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with\n",
      "an input that provides further context.\n",
      "Write a response that appropriately completes the request.\n",
      "Before answering, think carefully about the question and create a step-by\n",
      "step chain of thoughts to ensure a logical and accurate response.\n",
      "### Instruction:\n",
      "You are an expert in sentiment analysis with advanced knowledge in understanding\n",
      "and interpreting emotions from text.\n",
      "Please analyze the sentiment of the following text and output 0 (negative) or 1 (positive).\n",
      "### Text:\n",
      "hide new secretions from the parental units  \n",
      "### Response: \n",
      "<think> \n",
      "1. **Identify key sentiment-bearing words/phrases**:  \n",
      "   - \"hide\" implies concealment or secrecy, which often carries a negative connotation.  \n",
      "   - \"new secretions\" is an unusual and somewhat unsettling phrase, suggesting something unnatural or undesirable.  \n",
      "   - \"parental units\" is a detached and impersonal way to refer to parents, which can indicate emotional distance or resentment.  \n",
      "\n",
      "2. **Analyze contextual clues**:  \n",
      "   - The act of hiding something from parents suggests a lack of trust or fear of judgment.  \n",
      "   - The phrase \"new secretions\" is ambiguous but evokes a sense of something being hidden because it is inappropriate, embarrassing, or harmful.  \n",
      "   - The use of \"parental units\" instead of \"parents\" adds a cold, clinical tone, further distancing the speaker emotionally.  \n",
      "\n",
      "3. **Consider linguistic patterns**:  \n",
      "   - The choice of words like \"hide\" and \"secretions\" creates a sense of discomfort or unease.  \n",
      "   - The impersonal term \"parental units\" dehumanizes the parents, which can reflect negative feelings toward them.  \n",
      "   - The overall phrasing is indirect and evasive, reinforcing the negative sentiment.  \n",
      "\n",
      "4. **Synthesize overall sentiment**:  \n",
      "   - The text conveys a sense of secrecy, discomfort, and emotional detachment.  \n",
      "   - The negative connotations of the key words and the context suggest an underlying tension or conflict.  \n",
      "\n",
      "5. **Conclude with the final sentiment label**: \n",
      "</think> \n",
      "0<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:03.477652Z",
     "start_time": "2025-03-15T16:42:55.998400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference_example(text):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer([prompt_style.format(text, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=1200,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    return response[0].split(\"### Response:\")[1]\n",
    "\n",
    "\n",
    "# è®­ç»ƒå‰æ¨ç†ç¤ºä¾‹\n",
    "text = \"I absolutely loved the movie! The acting was superb and the storyline was captivating.\"\n",
    "print(\"è®­ç»ƒå‰æ¨ç†ç»“æœï¼š\")\n",
    "print(inference_example(text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå‰æ¨ç†ç»“æœï¼š\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out the sentiment of the given text. The text is: \"I absolutely loved the movie! The acting was superb and the storyline was captivating.\" Hmm, let's break this down.\n",
      "\n",
      "First, the user starts with \"I absolutely loved the movie!\" That's a strong positive expression. \"Loved\" is a very positive word. Then they mention the acting was superb. \"Superb\" is another positive adjective, indicating high praise for the acting. \n",
      "\n",
      "Next, they say the storyline was captivating. \"Captivating\" implies something that holds attention and is enjoyable, which is another positive aspect. So, all the words used here are positive and express approval and enjoyment.\n",
      "\n",
      "I don't see any negative words or phrases in there. There's no criticism or negative feedback. The user is clearly expressing enthusiasm and satisfaction with the movie.\n",
      "\n",
      "So, considering all these points, the sentiment should definitely be positive. Therefore, the score should be 1.\n",
      "</think>\n",
      "\n",
      "1<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:12.903737Z",
     "start_time": "2025-03-15T16:43:03.542651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:20.769353Z",
     "start_time": "2025-03-15T16:43:12.969188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
    "        # warmup_steps=5,\n",
    "        # max_steps=60, #max_steps=60ï¼šæ€»å…±è®­ç»ƒ 60 æ­¥ï¼ˆé€‚ç”¨äºçŸ­æš‚æµ‹è¯•ï¼Œå®Œæ•´è®­ç»ƒä¸€èˆ¬ä½¿ç”¨ num_train_epochsï¼‰\n",
    "        num_train_epochs = 1,\n",
    "        warmup_ratio = 0.1,\n",
    "        learning_rate=2e-4,\n",
    "        # fp16=not is_bfloat16_supported(), #fp16=not is_bfloat16_supported()ï¼šå¦‚æœä¸æ”¯æŒ bfloat16ï¼Œå°±ä½¿ç”¨ fp16ã€‚\n",
    "        # bf16=is_bfloat16_supported(), #bf16=is_bfloat16_supported()ï¼šå¦‚æœæ”¯æŒ bfloat16ï¼Œå°±ä½¿ç”¨ bf16ã€‚\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",#é‡‡ç”¨ AdamW ä¼˜åŒ–å™¨çš„ 8-bit ç‰ˆæœ¬ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼Œæé«˜å¤§æ¨¡å‹è®­ç»ƒæ•ˆç‡ï¼‰\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\"  # å…³é—­ wandb\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.53 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10 | Num Epochs = 1 | Total steps = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/4,670,623,744 (0.90% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:32.913808Z",
     "start_time": "2025-03-15T16:43:20.831207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"è®­ç»ƒåæ¨ç†ç»“æœï¼š\")\n",
    "print(inference_example(text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒåæ¨ç†ç»“æœï¼š\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out the sentiment of the given text. The user provided a text: \"I absolutely loved the movie! The acting was superb and the storyline was captivating.\" My task is to determine whether the sentiment is negative (0) or positive (1). \n",
      "\n",
      "First, I'll break down the text. The phrase \"I absolutely loved the movie\" immediately stands out. \"Loved\" is a strong positive word. It expresses a high level of satisfaction or enthusiasm. \n",
      "\n",
      "Next, the user mentions the acting was superb. \"Superb\" is another positive adjective, indicating excellence or outstanding quality. This reinforces the positive sentiment.\n",
      "\n",
      "Then, the storyline is described as captivating. \"Captivating\" suggests that the story held the reader's attention and interest, which is another positive trait. \n",
      "\n",
      "Looking at the overall structure, the user is giving positive feedback on two key aspects of the movie: acting and storyline. There are no negative words or phrases that would indicate dissatisfaction or criticism. \n",
      "\n",
      "Additionally, the exclamation mark after \"I absolutely loved the movie\" adds an emotional emphasis, further highlighting the positive tone. \n",
      "\n",
      "Considering all these elements, the sentiment of the text is clearly positive. There are no signs of negativity or mixed feelings. The user is expressing enthusiasm and satisfaction, which strongly indicates a positive sentiment. \n",
      "\n",
      "Therefore, after analyzing each part of the text and considering the emotional cues, I can confidently assign a positive sentiment score of 1.\n",
      "</think>\n",
      "\n",
      "1<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:43:46.033231Z",
     "start_time": "2025-03-15T16:43:32.979552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # å‰ææ˜¯è®¾ç½®äº†tokençš„ç¯å¢ƒå˜é‡\n",
    "login(token=hf_token)\n",
    "new_model_local = \"DeepSeek-R1-Sentiment-COT\"\n",
    "model.save_pretrained(new_model_local)\n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "model.save_pretrained_merged(new_model_local, tokenizer, save_method=\"merged_16bit\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 522.53 out of 755.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 41.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained(new_model_local)\n\u001B[1;32m      3\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(new_model_local)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained_merged\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_model_local\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmerged_16bit\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/unsloth/save.py:1313\u001B[0m, in \u001B[0;36munsloth_save_pretrained_merged\u001B[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001B[0m\n\u001B[1;32m   1311\u001B[0m arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1312\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m-> 1313\u001B[0m \u001B[43munsloth_save_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marguments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1314\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m   1315\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/unsloth/save.py:721\u001B[0m, in \u001B[0;36munsloth_save_model\u001B[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001B[0m\n\u001B[1;32m    712\u001B[0m     hf_api\u001B[38;5;241m.\u001B[39mupload_folder(\n\u001B[1;32m    713\u001B[0m         folder_path \u001B[38;5;241m=\u001B[39m new_save_directory,\n\u001B[1;32m    714\u001B[0m         path_in_repo \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    718\u001B[0m         ignore_patterns \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.md\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    719\u001B[0m     )\n\u001B[1;32m    720\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 721\u001B[0m     \u001B[43minternal_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msave_pretrained_settings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;66;03m# Revert config back\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/transformers/modeling_utils.py:3032\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   3027\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m   3029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m safe_serialization:\n\u001B[1;32m   3030\u001B[0m     \u001B[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001B[39;00m\n\u001B[1;32m   3031\u001B[0m     \u001B[38;5;66;03m# joyfulness), but for now this enough.\u001B[39;00m\n\u001B[0;32m-> 3032\u001B[0m     \u001B[43msafe_save_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3033\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3034\u001B[0m     save_function(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file))\n",
      "File \u001B[0;32m~/miniconda3/envs/instruct_tuning/lib/python3.10/site-packages/safetensors/torch.py:286\u001B[0m, in \u001B[0;36msave_file\u001B[0;34m(tensors, filename, metadata)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[1;32m    256\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[1;32m    257\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[1;32m    258\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    259\u001B[0m ):\n\u001B[1;32m    260\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 286\u001B[0m     \u001B[43mserialize_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "new_model_online = \"MelodyOfTears/DeepSeek-R1-Sentiment-COT\"\n",
    "model.push_to_hub(new_model_online)\n",
    "tokenizer.push_to_hub(new_model_online)\n",
    "model.push_to_hub_merged(new_model_online, tokenizer, save_method=\"merged_16bit\")"
   ]
  }
 ]
}
